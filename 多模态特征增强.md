# 三、多模态特征增强
同一对象的不同模态表示具有独特且共同的语义信息。如果能够区分独特特征和共同特征，那么MRS的推荐性能和泛化能力可以得到显着提高。解耦表征学习（DRL）和对比学习（CL）被用来进行基于交互的特征增强。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/3f7f8f82-abfa-4126-aada-174608494844)  

## 3.1 解耦表征学习DRL
### 不同模态特征对于用户对项目的特定因素的偏好具有不同的重要性。然而，每种模态中不同因素的表示往往是纠缠在一起的，因此许多研究人员引入分解学习技术来挖掘用户偏好中的细致因素。例如 DICER、MacridVAE、CDR。   

### DICER: Content-Collaborative Disentanglement Representation Learning for Enhanced Recommendation 2020
从这些协作和基于内容的视角得到的未发现的用户偏好表示可能会由于相互影响而相互纠缠，从而导致次优性能和不稳定的推荐。因此，我们建议从用户行为数据和内容信息中分离表示。具体地说，我们提出了一种新的两级分解生成推荐模型（DICER），该模型支持内容协同分解和特征分解：  
对于内容协同分解，DICER根据内容和用户项交互的边际分布对特征进行分解，以确保每种类型的学习特征在统计上是独立的。  
对于特征分离，通过对Kullback-Leibler散度的分解，我们从理论上证明了每种类型中提取的特征在粒度级别上是分离的。  
此外，DICER利用同时解码内容和用户项交互的联合解码器来确保学习特征的高质量。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/952abc22-5705-4788-acd5-bc3a88e2eec0)  
（a）从用户项交互学习的协作特征和从内容信息（如图像）学习的内容特征都可以用于发现用户偏好。  
（b）DICER首先将用户隐含反馈xi分解为内容信息ci和内容分解后的协作信息x′i。然后在每种类型（协作或内容）中，我们进一步在粒度级别（特征分离）分离学习的特征，以提高用户和项目表示的能力。  

### MacridVAE: Learning Disentangled Representations for Recommendation 2019
推荐系统中的用户行为数据是由用户决策过程背后诸多潜在因素的复杂交互驱动的。这些因素是高度纠缠的，并且可以从控制用户意图的高级因素到表征用户在执行意图时的偏好的低级因素。学习表征，发现和理清这些潜在的因素可以带来增强的鲁棒性，可解释性和可控性。然而，从用户行为中学习这些分离的表示是一个挑战，并且在很大程度上被现有文献所忽视。  
我们提出宏-微分离变分自动编码器（MacridVAE）来学习基于用户行为的分离表示。我们的方法明确地模拟了宏观和微观因素的分离，并在每个层次上进行分离。通过识别与用户意图相关联的高层概念，并分别学习用户对不同概念的偏好，实现宏观解纠缠。然后，从信息论的角度解释VAEs[27，44]得到的微观解纠缠正则化子得到加强，以迫使每个单独的维度反映一个独立的微观因素。提出了一种波束搜索策略，该策略通过寻找平滑的轨迹来处理稀疏离散观测值和密集连续表示之间的冲突，以研究每个孤立维度的可解释性。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/f61a6c28-8445-4a47-a379-d892643dbc81)  
通过学习一组原型，推断出与每个项目相关的用户意图，然后分别捕获用户对不同意图的偏好，实现宏观解纠缠。  
通过放大KL散度实现微观解纠缠，从KL散度中可以分离出惩罚总相关性的项，因子为β。  

### 多模态推荐是从复杂的、高度纠缠的多模态数据中发现各种隐藏因素形成的有用信息。
### MDR: MULTIMODAL DISENTANGLED REPRESENTATION FOR RECOMMENDATION  2021
**MDR提出一个多模态解纠缠推荐能够很好地从不同模态中学习携带互补和标准信息的解纠缠表示。**  

发现多模态数据中各种隐藏因素形成的有用信息对于提高模型性能和推荐解释性具有重要意义。这些隐藏在多模态数据中的因素以一种复杂的方式高度纠缠，在推荐的表示学习过程中暴露它们的纠缠是一个巨大的挑战。然而，现有的文献只关注单峰数据，没有揭示多峰数据中复杂和纠缠的因素。  
本文首次研究了弱监督下的多模态解纠缠推荐问题。提出了一种多模态解纠缠推荐（MDR）模型，该模型能够很好地从不同的模式中学习包含互补信息和公共信息的解纠缠表示，从而提高推荐的准确性和表示的可解释性。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/23f379c4-0a23-4796-b143-a41300fa695b)  
更具体地说，我们采用定制的编码、融合和解码结构以及专门设计的解纠缠方法来获得多模态解纠缠表示。在编码过程中，输入的图像和文本分别用特定的编码器编码成单峰表示。然后对得到的单峰表示进行解纠缠融合，得到多峰表示。  
在解码过程中，利用重构损失来保持足够的输入信息，最小化因子间的互信息来保证解纠缠，引入弱监督来进一步提高表示的可解释性。我们利用正则化的信息约束来保证所学习的多模态解纠缠表示的可解释性。  

### DMRL:Disentangled Multimodal Representation Learning for Recommendation 2022
**DMRL考虑了不同模态特征对每个解纠缠因子的不同贡献，以捕获用户偏好。**  
许多多模态推荐系统被提出来利用与用户或项目（如用户评论和项目图像）相关的丰富边信息来学习更好的用户和项目表示以提高推荐性能。心理学研究表明，用户在组织信息的方式上存在个体差异。因此，对于项目的特定因素（例如外观或质量），不同模式的特征对用户具有不同的重要性。然而，现有的方法忽略了这样一个事实，即不同的模式对用户对项目的各种因素的偏好有不同的贡献。  
鉴于此，本文提出了一种新的非纠缠多模态表示学习（DMRL）推荐模型，该模型能够捕获用户对用户偏好建模中各个因素上不同模态的注意。特别地，我们采用了一种非纠缠表示技术，以确保每个模态中不同因素的特征是相互独立的。然后设计一个多模态注意机制来捕获用户对每个因素的模态偏好。根据注意机制得到的估计权重，结合不同模式下用户对目标项目各因素的偏好得分，提出推荐。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/5be8cf42-9321-4a39-8b29-a797ce080457)
在我们的模型中，用户对一个项目的偏好是通过聚合她对不同因素的所有模态特征的加权偏好来预测的。为了使每个因素具有鲁棒性和独立的表示，我们采用了一种分离表示技术，以确保每个模态中不同因素的特征是相互独立的。  
基于不同因素的分离表示，我们设计了一种多模态注意机制来捕捉用户对不同因素的模态偏好。  
最后，对于偏好预测，在给定用户和目标项目的情况下，首先估计用户对每个模态中每个因子的偏好得分，然后利用注意机制得到的估计权重将不同模态中所有因子的得分线性组合。为此，该模型基于多模态表示的分离因子，结合用户的个人模态偏好，对用户的个人模态偏好进行分析。

### PAMD: Modality Matches Modality: Pretraining Modality-Disentangled Item Representations for Recommendation 2022
**PAMD设计了一种分离编码器，在自动保留其模态特征的同时提取其模态共同特征。此外，对比学习保证了分离模态表征之间的一致性和差异性。**  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/e3de034a-52b4-40dd-afa9-11fb6090243b)  
通过实例说明了推荐场景中的模态对齐问题。蓝色背景中的特征是对齐的模态特征，橙色和绿色背景中的特征分别是项目图像和描述的模态特征。  
最近的研究表明，结合文本和视觉信息可以有效地解决推荐场景中的稀疏性问题。为了融合这些有用的异构模态信息，对齐这些信息是模态鲁棒特征学习和语义理解的必要前提。然而，现有的研究主要集中在解决跨模式的知识学习问题，而忽略了每个模式的具体特征，这必然会降低推荐性能。为此，我们提出了一个预训练框架PAMD，即预训练模态分离表示模型。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/5ab2ef5c-d337-4e50-be5a-b7f5c908caa0)  
PAMD包含两个部分：一个分离编码器将多模态信息分解为模态公共特征和模态特定特征，另一个具有自监督目标的对比学习以保证这些分解特征之间的精确对齐和分离。  
具体地说，考虑到与项目相关的视觉和文本模态，PAMD利用现成的VGG19和Glove分别将模态嵌入到连续嵌入空间中。基于这些原始的异构表示，PAMD首先将它们输入一个分离的编码器，其中每个模态数据被分解为两个表示：一个模态公共表示，跨模态共享语义；一个模态特定表示，包含该模态通道中的独特特征。  
在此基础上，进一步设计了对比学习，以确保这些分解表示之间的精确对齐和分离。由于自监督训练的优点，我们的PAMD可以在没有监督信号的情况下自动学习模态分离项表示，并且被证明比现有的最新的推荐模型更有效。  

### SEM-MacridVAE: Disentangled Representation Learning for Recommendation. 2023
**与MacridVAE相比，SEM-MacridVAE在学习用户行为的分离表示时考虑了项目语义信息。**  
缺pdf

## 3.2 对比学习
与 DRL 不同，对比学习方法通过数据增强来增强表示，这也有助于处理稀疏问题。CL损失函数主要用于模态对齐和增强正负样本之间的深层特征信息。  

### MCPTR: Multi-Modal Contrastive Pre-training for Recommendation 2022
**MCPTR提出了一种新的CL损失，使得同一项目的不同模态表示具有语义相似性。**  
对预训练好的多模态嵌入进行微调，建立了现有的推荐模型。如图所示，除了描述文本、图像和评论文本之外，我们还基于交互关系构造了两个同质图。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/e0c37df2-57a7-4793-b1aa-4c49c318feef)  
因此，在本文中，我们考虑两种用户模式：评论文本和用户图，三种项目模式：描述文本、图像和项目图。我们分别使用文本编码器、图像编码器和图形编码器来获得每个模态的表示。对于用户，我们提出了模态内聚合和模态间聚合来融合多个模态。模态内融合的目的是融合多个评论文本，模态间融合的目的是获得多模态用户表示。对于项目，我们还应用跨模态聚合来获得多模态项目表示。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/1d2e417f-a11f-41f9-8287-842d5761c1b0)  
此外，描述文本和图像对于同一项目也是相辅相成的。其中一个可以作为另一个有希望的监督。以上图为例，每个项目的描述是从文本模特的角度表现出来的，图像是从视觉模态的角度表现出来的。**这两种模态的语义是相似的。为了有效地捕获这一信号，我们提出了一种自监督对比学习方法，该方法将项目的文本和视觉模式对齐。** 在获得用户和项目的多模态表示之后，我们使用二元交叉熵损失函数来捕获它们之间的潜在相关性。  

### GHMFC: Multimodal Entity Linking with Gated Hierarchical Fusion and Contrastive Training 2022
**GHMFC基于图神经网络的实体嵌入表示构造了两个对比学习模块。两个CL损失函数在两个方向上，即文本到图像和图像到文本。**  
以往知识图中的实体链接方法大多将文本引用链接到相应的实体。然而，当文本太短而无法提供足够的上下文时，它们在处理大量多模态数据方面存在不足。因此，我们设想引入其他模态的有价值信息，并提出了一种基于门控层次多模态融合和对比训练的多模态实体连接方法（GHMFC）。  
首先，为了发现细粒度的模态间相关性，GHMFC通过多模态共同注意机制（文本引导的视觉注意和视觉引导的文本注意）提取文本和视觉共同注意的层次特征。前者在文本信息的引导下获得加权的视觉特征。相反，后者在视觉信息的引导下产生加权的文本特征。  
然后，利用门控融合方法评估不同模态层次特征的重要性，并将其集成到提及的最终多模态表示中。随后，设计了两种对比损失的对比训练，以学习更多的通用多模态特征并降低噪声。最后，通过计算KGs中引用和实体表示之间的余弦相似度来选择链接实体。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/2706cbe5-3725-430d-a438-0faca3c6714e)  

### Cross-CBR: Cross-view Contrastive Learning for Bundle Recommendation 2022
**Cross-CBR提出了一个CL损失，以从捆绑视图和项目视图对齐图形表示。**  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/16b7bc67-2a73-4cdc-80d2-ede2089b2f71)  
提出了一种简单而有效的束推荐器CrossCBR，通过跨视图对比学习对两个视图之间的协同关联进行建模。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/3488e691-389b-45b5-9d67-da59a3b2b44c)  

### MICRO: Latent Structure Mining with Contrastive Modality Fusion for Multimedia Recommendation 2022
**MICRO关注共享模态信息和特定模态信息。**   
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/561fc6e9-9cc2-4797-9251-e43304e6ebdb)  
设计了一种新的多模态对比框架，通过使模态感知表示和多模态融合表示紧密结合，实现细粒度多模态融合。

### CMCKG: Cross-modal Knowledge Graph Contrastive Learning for Machine Learning Method Recommendation 2022
**通过对比损失知识图，从描述属性和结构链接信息中获取实体嵌入。**  
将描述属性和结构连接信息作为两种模式，通过最大化描述视图和结构视图之间的一致性来学习信息节点表示。  
我们将传统的知识图关系分为描述性属性和结构连接，以考虑更多反映实体差异性的描述性信息。  
提出了一种跨模态知识图对比学习方法，该方法从不同模态中获取实体特征并相互监督以获得更有效的实体表示。  
![image](https://github.com/NanGongNingYi/Multimodal-Recommendation-Papers/assets/61775768/f37c69c7-e62d-48bc-9f90-ace638759f87)  
左边部分说明了描述属性和给定节点的结构连接之间的区别。右侧代表描述和结构观点的跨模态对比学习。  

### HCGCN: 



















