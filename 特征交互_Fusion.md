# 二、Fusion
融合不同的多模态信息，与bridge相比，融合更关注项目之间的多模态内部关系。  
它可以灵活地融合不同权重和焦点的多模态信息。  
注意机制是应用最为广泛的特征融合。
## 2.1 粗粒度注意力。
一些模型应用注意力机制在粗粒度级别融合来自多种模式的信息。  
### 注：将多模态信息分为用户侧和项目侧，包括各自的id信息和side信息：UVCAN、MCPTR。  
### UVCAN: User-Video Co-Attention Network for Personalized Micro-video Recommendation 2019
UVCAN将多模态信息分为用户侧和物品侧，包括各自的id信息和side信息。它利用用户侧的多模态数据，通过自注意力生成项目侧的融合权重。  
![image](https://github.com/NanGongNingYi/-/assets/61775768/cb0033f3-14a5-4cd8-bc0d-7e57bad42769)
协同过滤的插图通过三步推理堆叠了注意力网络框架来探索用户对微视频特征的关注。 

### MCPTR: Multi-Modal Contrastive Pre-training for Recommendation 2022
MCPTR建议并行合并项目和用户信息。  
方法包含两个过程：预训练和微调。  
**在预训练阶段**，我们提出了一种基于辅助信息和隐式反馈矩阵 R 的多模态对比表示模型。具体来说，我们提出的预训练模型包含两个组成部分：用户建模和项目建模。  
**在用户建模部分**，我们首先使用文本编码器来获取每个评论文本的表示，然后使用模内聚合来获取用户的评论嵌入。接下来，应用图编码器来捕获齐次图 Gu 的结构信息。对于这两种不同的模态信息，我们开发了模态间聚合以获得用户的多模态表示。  
**在项目建模中**，我们利用文本编码器、图像编码器和图编码器对每个项目的描述文本、图像和齐次图 Gi 进行编码。然后，我们应用模间聚合来获得项目的多模态表示。此外，由于对于同一项目，描述文本和图像信息是互补的，因此它们具有相似的语义。我们开发了一种自我监督的对比学习方法来调整它们之间的表示。  
最后，根据[14]，我们采用基于反馈矩阵R的二元交叉熵损失函数来捕获目标用户u与其对应的目标项目i的潜在相关性。**在微调过程中**，现有的推荐模型利用预先训练的用户/项目嵌入作为初始化，并仅基于反馈矩阵 R 微调这些嵌入。  
![image](https://github.com/NanGongNingYi/-/assets/61775768/d1deaaab-da29-486e-8ab0-8b3e60508da9)



### 注：CMBF、MML、MCPTR、HCGCN引入交叉注意力机制来分别共同学习图像和文本模态的语义信息。  
### CMBF:

### MML: 

### MCPTR: 

### HCGCN: 

